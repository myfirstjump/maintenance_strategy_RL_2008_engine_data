0830_exp_1
        ### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -100
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -1000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 200

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 1000#0
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 150000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
Trigger: max_episode_steps
Step count: 1000, do_nothing count: 942, lubrication count: 54, replacment count: 4
987222: done 1140 games, reward 178.2, eps 0.01, speed 123.7498563511102 f/s
Trigger: max_episode_steps
Step count: 1000, do_nothing count: 938, lubrication count: 60, replacment count: 2
988222: done 1141 games, reward 192.4, eps 0.01, speed 124.43598449202113 f/s
Best reward updated 191.72 -> 192.4
Trigger: max_episode_steps
Step count: 1000, do_nothing count: 940, lubrication count: 58, replacment count: 2
989222: done 1142 games, reward 204.64, eps 0.01, speed 124.56962887059294 f/s
Best reward updated 192.4 -> 204.64
Solved in 989222 frames!



0831_exp_2
        ### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -200
        self.DO_NOTHING_REWARD = 2
        self.FAILURE_REWARD = -2000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 400

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 150000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01

		Step count: 1000, do_nothing count: 957, lubrication count: 38, replacment count: 5
		335379: done 371 games, reward 377.39, eps 0.01, speed 124.77593870726622 f/s
		Best reward updated 362.0 -> 377.39
		Trigger: Engine Failure!!!
		Step count: 147, do_nothing count: 147, lubrication count: 0, replacment count: 0
		335526: done 372 games, reward 351.75, eps 0.01, speed 118.58457076365315 f/s
		Trigger: max_episode_steps
		Step count: 1000, do_nothing count: 945, lubrication count: 53, replacment count: 2
		336526: done 373 games, reward 381.22, eps 0.01, speed 125.44988499765209 f/s
		Best reward updated 377.39 -> 381.22
		Trigger: max_episode_steps
		Step count: 1000, do_nothing count: 926, lubrication count: 71, replacment count: 3
		337526: done 374 games, reward 404.75, eps 0.01, speed 124.79013480820575 f/s
		Best reward updated 381.22 -> 404.75
		Solved in 337526 frames!

0831_exp_3
	
		nn.Linear(obs_len * previous_state_used, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, actions_n)
			
		        ### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -200
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -2000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 400

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 150000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
		Step count: 1000, do_nothing count: 925, lubrication count: 74, replacment count: 1
		839705: done 945 games, reward -342.64, eps 0.01, speed 127.84820366784969 f/s
		Trigger: max_episode_steps
		Step count: 1000, do_nothing count: 954, lubrication count: 40, replacment count: 6
		840705: done 946 games, reward -326.55, eps 0.01, speed 123.00652013411255 f/s
		Trigger: max_episode_steps
		Step count: 1000, do_nothing count: 937, lubrication count: 61, replacment count: 2
		841705: done 947 games, reward -306.29, eps 0.01, speed 114.0659068226865 f/s
		Trigger: max_episode_steps
		Step count: 1000, do_nothing count: 948, lubrication count: 47, replacment count: 5
		842705: done 948 games, reward -308.42, eps 0.01, speed 125.3376310406477 f/s
	
0831_exp_4
		self.previous_p_times = 5 ### RL states --> 10
		
		Step count: 1000, do_nothing count: 958, lubrication count: 38, replacment count: 4
		686023: done 794 games, reward -415.69, eps 0.01, speed 99.09612409375497 f/s
		Trigger: max_episode_steps
		{'Engine': 118, 'Engine_max_cycle': 188, 'Action': <Actions.Nothing: 0>, 'offset': 80, 'state_range': '[70, 80]'}
		Step count: 1000, do_nothing count: 951, lubrication count: 44, replacment count: 5
		687023: done 795 games, reward -416.05, eps 0.01, speed 97.75907639704994 f/s
		Trigger: max_episode_steps
		{'Engine': 144, 'Engine_max_cycle': 272, 'Action': <Actions.Nothing: 0>, 'offset': 120, 'state_range': '[110, 120]'}
		Step count: 1000, do_nothing count: 955, lubrication count: 41, replacment count: 4
		688023: done 796 games, reward -412.39, eps 0.01, speed 95.61458579818259 f/s
		
0831_exp_5
		self.previous_p_times = 5 ### RL states
		### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -200
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -2000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 50

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 5e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 150000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
		Trigger: max_episode_steps
		{'Engine': 99, 'Engine_max_cycle': 263, 'Action': <Actions.Lubrication: 1>, 'offset': 5, 'state_range': '[0, 5]'}
		Step count: 1000, do_nothing count: 830, lubrication count: 167, replacment count: 3
		3347705: done 6440 games, reward -1925.95, eps 0.01, speed 123.68961982825611 f/s
		Trigger: Engine Failure!!!
		Step count: 288, do_nothing count: 172, lubrication count: 115, replacment count: 1
		3347993: done 6441 games, reward -1934.13, eps 0.01, speed 120.56762423322712 f/s
		Trigger: Engine Failure!!!
		Step count: 388, do_nothing count: 318, lubrication count: 68, replacment count: 2
		3348381: done 6442 games, reward -1940.93, eps 0.01, speed 114.49679929024774 f/s

0903_exp_1

		### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -200
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -2000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 200
        self.previous_p_times = 5 ### RL states

        self.GAMMA = 0.99
        self.BATCH_SIZE = 16
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 150000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
		nn.Linear(obs_len * previous_state_used, 512),
		nn.ReLU(),
		nn.Linear(512, 256),
		nn.ReLU(),
		nn.Linear(256, 128),
		nn.ReLU(),
		nn.Linear(128, actions_n)
		
		Trigger: max_episode_steps
		{'Engine': 131, 'Engine_max_cycle': 263, 'Action': <Actions.Nothing: 0>, 'offset': 147, 'state_range': '[142, 147]'}
		Step count: 1000, do_nothing count: 952, lubrication count: 45, replacment count: 3
		858101: done 988 games, reward -346.43, eps 0.01, speed 126.97614247615986 f/s
		Trigger: max_episode_steps
		{'Engine': 216, 'Engine_max_cycle': 190, 'Action': <Actions.Nothing: 0>, 'offset': 77, 'state_range': '[72, 77]'}
		Step count: 1000, do_nothing count: 938, lubrication count: 60, replacment count: 2
		859101: done 989 games, reward -325.67, eps 0.01, speed 131.34109327366275 f/s
	
0903_exp_2
		減小網路結構
		
		### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -200
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -2000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 200
        self.previous_p_times = 5 ### RL states

        self.GAMMA = 0.99
        self.BATCH_SIZE = 16
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 150000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
		nn.Linear(obs_len * previous_state_used, 256),
		nn.ReLU(),
		nn.Linear(256, 256),
		nn.ReLU(),
		nn.Linear(256, actions_n),
		
		Trigger: max_episode_steps
		{'Engine': 94, 'Engine_max_cycle': 209, 'Action': <Actions.Nothing: 0>, 'offset': 145, 'state_range': '[140, 145]'}
		Step count: 1000, do_nothing count: 950, lubrication count: 46, replacment count: 4
		824462: done 931 games, reward -470.63, eps 0.01, speed 162.69259143337806 f/s
		Trigger: Engine Failure!!!
		Step count: 437, do_nothing count: 422, lubrication count: 14, replacment count: 1
		824899: done 932 games, reward -490.57, eps 0.01, speed 144.95281829110425 f/s
		Trigger: max_episode_steps
		{'Engine': 203, 'Engine_max_cycle': 225, 'Action': <Actions.Nothing: 0>, 'offset': 29, 'state_range': '[24, 29]'}
		Step count: 1000, do_nothing count: 945, lubrication count: 50, replacment count: 5
		825899: done 933 games, reward -496.0, eps 0.01, speed 165.10595636991778 f/s

0903_exp_3
		提高batch_size -> 64

        ### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -200
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -2000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 200
        self.previous_p_times = 5 ### RL states

        self.GAMMA = 0.99
        self.BATCH_SIZE = 64
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 150000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
		Trigger: max_episode_steps
		{'Engine': 54, 'Engine_max_cycle': 249, 'Action': <Actions.Nothing: 0>, 'offset': 59, 'state_range': '[54, 59]'}
		Step count: 1000, do_nothing count: 940, lubrication count: 57, replacment count: 3
		3770320: done 4044 games, reward -260.53, eps 0.01, speed 141.5126579701059 f/s
		Trigger: max_episode_steps
		{'Engine': 119, 'Engine_max_cycle': 242, 'Action': <Actions.Lubrication: 1>, 'offset': 82, 'state_range': '[77, 82]'}
		Step count: 1000, do_nothing count: 922, lubrication count: 77, replacment count: 1
		3771320: done 4045 games, reward -255.67, eps 0.01, speed 137.68302694514963 f/s

0903_exp_4
		Epilson decay加快 (EPSILON_DECAY_LAST_FRAME = 150000 ---> 80000)
		回饋值目標調降 200 --> 0
		
		### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -200
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -2000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 0
        self.previous_p_times = 5 ### RL states

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 100000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
		
		Trigger: max_episode_steps
		{'Engine': 92, 'Engine_max_cycle': 207, 'Action': <Actions.Nothing: 0>, 'offset': 126, 'state_range': '[121, 126]'}
		Step count: 1000, do_nothing count: 935, lubrication count: 63, replacment count: 2
		429415: done 484 games, reward -377.16, eps 0.01, speed 164.95392872587556 f/s
		Trigger: max_episode_steps
		{'Engine': 102, 'Engine_max_cycle': 222, 'Action': <Actions.Nothing: 0>, 'offset': 46, 'state_range': '[41, 46]'}
		Step count: 1000, do_nothing count: 940, lubrication count: 55, replacment count: 5
		430415: done 485 games, reward -362.51, eps 0.01, speed 157.26025548252602 f/s
		
0903_exp_5
		提升failure之懲罰量 -2000 --> -5000
		模型調整回雙層512
		epilson遞減速度稍緩一點 80000 -> 100000
		
        ### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -200
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -5000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 50
        self.previous_p_times = 5 ### RL states

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 100000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01		
		
		nn.Linear(obs_len * previous_state_used, 512),
		nn.ReLU(),
		nn.Linear(512, 512),
		nn.ReLU(),
		nn.Linear(512, actions_n),

0903_exp_6
		
		依照Maxim的範例，Ch10他的epilson遞減應該比replay initial大10倍
		epilson遞減速度再緩 100000 --> 500000 多一些探索

        ### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -200
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -5000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 50
        self.previous_p_times = 5 ### RL states

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 500000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01		
		
		Trigger: max_episode_steps
		{'Engine': 26, 'Engine_max_cycle': 210, 'Action': <Actions.Lubrication: 1>, 'offset': 5, 'state_range': '[0, 5]'}
		Step count: 1000, do_nothing count: 829, lubrication count: 168, replacment count: 3
		789868: done 823 games, reward -725.07, eps 0.01, speed 100.13849338828248 f/s
		Trigger: max_episode_steps
		{'Engine': 109, 'Engine_max_cycle': 160, 'Action': <Actions.Nothing: 0>, 'offset': 89, 'state_range': '[84, 89]'}
		Step count: 1000, do_nothing count: 893, lubrication count: 103, replacment count: 4
		790868: done 824 games, reward -734.31, eps 0.01, speed 101.33603105054992 f/s
		
		
0903_exp_7
		Replacement的回饋應該要提升 -200 ---> -150
		因為每支引擎的cycle數平均也才210，如果-200這樣會與運行獎勵抵銷。
		
		### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -150
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -5000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 50
        self.previous_p_times = 5 ### RL states

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 500000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
		nn.Linear(obs_len * previous_state_used, 512),
		nn.ReLU(),
		nn.Linear(512, 512),
		nn.ReLU(),
		nn.Linear(512, actions_n),		
		
		
		
		
		
0903_exp_8	

		1. 再提高replacement回饋
		2. tgt sync頻率提升
		
		### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -100
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -5000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 100
        self.previous_p_times = 5 ### RL states

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 5000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 500000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
		Trigger: max_episode_steps
		{'Engine': 112, 'Engine_max_cycle': 226, 'Action': <Actions.Nothing: 0>, 'offset': 141, 'state_range': '[136, 141]'}
		Step count: 1000, do_nothing count: 940, lubrication count: 57, replacment count: 3
		1008371: done 1093 games, reward -715.47, eps 0.01, speed 105.87509139079293 f/s
		Trigger: max_episode_steps
		{'Engine': 120, 'Engine_max_cycle': 170, 'Action': <Actions.Nothing: 0>, 'offset': 31, 'state_range': '[26, 31]'}
		Step count: 1000, do_nothing count: 956, lubrication count: 40, replacment count: 4
		1009371: done 1094 games, reward -716.35, eps 0.01, speed 105.04420051779982 f/s

0903_exp_9
		tgt sync頻率回復到 5000 -> 10000
		
		### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -100
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -5000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 100
        self.previous_p_times = 5 ### RL states

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 500000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
		nn.Linear(obs_len * previous_state_used, 512),
		nn.ReLU(),
		nn.Linear(512, 512),
		nn.ReLU(),
		nn.Linear(512, actions_n),
		
		
0905_exp_1
		
		SimpleDNN --> SimpleDNN_small  (參考Skordilis and Moghaddass 2020)
		另可嘗試 optimizer: RMSprop lambda 0.001 tau 0.9
		
        
        ### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -100
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -5000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 100
        self.previous_p_times = 5 ### RL states

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 500000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01

            nn.Linear(obs_len * previous_state_used, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, actions_n),
			
		Trigger: max_episode_steps
		{'Engine': 42, 'Engine_max_cycle': 241, 'Action': <Actions.Nothing: 0>, 'offset': 45, 'state_range': '[40, 45]'}
		Step count: 1000, do_nothing count: 943, lubrication count: 52, replacment count: 5
		858909: done 903 games, reward 49.38, eps 0.01, speed 226.90518512178986 f/s
		Best reward updated -1.54 -> 49.38
		Trigger: max_episode_steps
		{'Engine': 207, 'Engine_max_cycle': 278, 'Action': <Actions.Nothing: 0>, 'offset': 58, 'state_range': '[53, 58]'}
		Step count: 1000, do_nothing count: 920, lubrication count: 79, replacment count: 1
		859909: done 904 games, reward 101.84, eps 0.01, speed 219.34904286498664 f/s
		Best reward updated 49.38 -> 101.84

	不知為何，Tensorboard沒有辦法顯示。
	

0910_exp_4
		測試nvidia-docker
		
		### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -100
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -5000

        ### RL strategy settings
        self.DEVICE = 'cuda' ### "cpu"
        self.MEAN_REWARD_BOUND = 100
        self.previous_p_times = 5 ### RL states

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 500000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
		Trigger: max_episode_steps
		{'Engine': 207, 'Engine_max_cycle': 278, 'Action': <Actions.Nothing: 0>, 'offset': 20, 'state_range': '[15, 20]'}
		Step count: 1000, do_nothing count: 903, lubrication count: 95, replacment count: 2
		981535: done 1020 games, reward 51.16, eps 0.01, speed 131.94365275601945 f/s
		Trigger: max_episode_steps
		{'Engine': 126, 'Engine_max_cycle': 241, 'Action': <Actions.Nothing: 0>, 'offset': 46, 'state_range': '[41, 46]'}
		Step count: 1000, do_nothing count: 913, lubrication count: 86, replacment count: 1
		982535: done 1021 games, reward 51.91, eps 0.01, speed 131.95336601412058 f/s
		Trigger: max_episode_steps
		{'Engine': 188, 'Engine_max_cycle': 246, 'Action': <Actions.Nothing: 0>, 'offset': 27, 'state_range': '[22, 27]'}
		Step count: 1000, do_nothing count: 909, lubrication count: 89, replacment count: 2
		983535: done 1022 games, reward 102.67, eps 0.01, speed 130.45390126244968 f/s
		Best reward updated 60.21 -> 102.67
		Solved in 983535 frames!
		
		
0913_valid
		過程發現，到達max_episode_steps=1000時，_offset總是136
		
		Step後step_counter: 993
		Step後offset: 140
		Step後step_counter: 994
		Step後offset: 130
		Step後step_counter: 995
		Step後offset: 131
		Step後step_counter: 996
		Step後offset: 132
		Step後step_counter: 997
		Step後offset: 133
		Step後step_counter: 998
		Step後offset: 134
		Step後step_counter: 999
		Step後offset: 135
		Step後step_counter: 1000
		Step後offset: 136
		Trigger: max_episode_steps
		
		發現將cycle數作為變數的話，每次固定140就會持續潤滑。
		建議若要讓NN學會隱藏狀態(hidden states)的話，不要加入cycle一起訓練!
		
		self.features_num = 25 --> 24
		
		
0913_exp_1_gpu