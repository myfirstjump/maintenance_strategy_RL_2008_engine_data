0830_exp_1
        ### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -100
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -1000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 200

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 1000#0
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 150000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
Trigger: max_episode_steps
Step count: 1000, do_nothing count: 942, lubrication count: 54, replacment count: 4
987222: done 1140 games, reward 178.2, eps 0.01, speed 123.7498563511102 f/s
Trigger: max_episode_steps
Step count: 1000, do_nothing count: 938, lubrication count: 60, replacment count: 2
988222: done 1141 games, reward 192.4, eps 0.01, speed 124.43598449202113 f/s
Best reward updated 191.72 -> 192.4
Trigger: max_episode_steps
Step count: 1000, do_nothing count: 940, lubrication count: 58, replacment count: 2
989222: done 1142 games, reward 204.64, eps 0.01, speed 124.56962887059294 f/s
Best reward updated 192.4 -> 204.64
Solved in 989222 frames!



0831_exp_2
        ### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -200
        self.DO_NOTHING_REWARD = 2
        self.FAILURE_REWARD = -2000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 400

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 150000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01

		Step count: 1000, do_nothing count: 957, lubrication count: 38, replacment count: 5
		335379: done 371 games, reward 377.39, eps 0.01, speed 124.77593870726622 f/s
		Best reward updated 362.0 -> 377.39
		Trigger: Engine Failure!!!
		Step count: 147, do_nothing count: 147, lubrication count: 0, replacment count: 0
		335526: done 372 games, reward 351.75, eps 0.01, speed 118.58457076365315 f/s
		Trigger: max_episode_steps
		Step count: 1000, do_nothing count: 945, lubrication count: 53, replacment count: 2
		336526: done 373 games, reward 381.22, eps 0.01, speed 125.44988499765209 f/s
		Best reward updated 377.39 -> 381.22
		Trigger: max_episode_steps
		Step count: 1000, do_nothing count: 926, lubrication count: 71, replacment count: 3
		337526: done 374 games, reward 404.75, eps 0.01, speed 124.79013480820575 f/s
		Best reward updated 381.22 -> 404.75
		Solved in 337526 frames!

0831_exp_3
	
		nn.Linear(obs_len * previous_state_used, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, actions_n)
			
		        ### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -200
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -2000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 400

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 150000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
		Step count: 1000, do_nothing count: 925, lubrication count: 74, replacment count: 1
		839705: done 945 games, reward -342.64, eps 0.01, speed 127.84820366784969 f/s
		Trigger: max_episode_steps
		Step count: 1000, do_nothing count: 954, lubrication count: 40, replacment count: 6
		840705: done 946 games, reward -326.55, eps 0.01, speed 123.00652013411255 f/s
		Trigger: max_episode_steps
		Step count: 1000, do_nothing count: 937, lubrication count: 61, replacment count: 2
		841705: done 947 games, reward -306.29, eps 0.01, speed 114.0659068226865 f/s
		Trigger: max_episode_steps
		Step count: 1000, do_nothing count: 948, lubrication count: 47, replacment count: 5
		842705: done 948 games, reward -308.42, eps 0.01, speed 125.3376310406477 f/s
	
0831_exp_4
		self.previous_p_times = 5 ### RL states --> 10
		
		Step count: 1000, do_nothing count: 958, lubrication count: 38, replacment count: 4
		686023: done 794 games, reward -415.69, eps 0.01, speed 99.09612409375497 f/s
		Trigger: max_episode_steps
		{'Engine': 118, 'Engine_max_cycle': 188, 'Action': <Actions.Nothing: 0>, 'offset': 80, 'state_range': '[70, 80]'}
		Step count: 1000, do_nothing count: 951, lubrication count: 44, replacment count: 5
		687023: done 795 games, reward -416.05, eps 0.01, speed 97.75907639704994 f/s
		Trigger: max_episode_steps
		{'Engine': 144, 'Engine_max_cycle': 272, 'Action': <Actions.Nothing: 0>, 'offset': 120, 'state_range': '[110, 120]'}
		Step count: 1000, do_nothing count: 955, lubrication count: 41, replacment count: 4
		688023: done 796 games, reward -412.39, eps 0.01, speed 95.61458579818259 f/s
		
0831_exp_5
		self.previous_p_times = 5 ### RL states
		### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -200
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -2000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 50

        self.GAMMA = 0.99
        self.BATCH_SIZE = 32
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 5e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 150000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
		Trigger: max_episode_steps
		{'Engine': 99, 'Engine_max_cycle': 263, 'Action': <Actions.Lubrication: 1>, 'offset': 5, 'state_range': '[0, 5]'}
		Step count: 1000, do_nothing count: 830, lubrication count: 167, replacment count: 3
		3347705: done 6440 games, reward -1925.95, eps 0.01, speed 123.68961982825611 f/s
		Trigger: Engine Failure!!!
		Step count: 288, do_nothing count: 172, lubrication count: 115, replacment count: 1
		3347993: done 6441 games, reward -1934.13, eps 0.01, speed 120.56762423322712 f/s
		Trigger: Engine Failure!!!
		Step count: 388, do_nothing count: 318, lubrication count: 68, replacment count: 2
		3348381: done 6442 games, reward -1940.93, eps 0.01, speed 114.49679929024774 f/s

0903_exp_1

		### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -200
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -2000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 200
        self.previous_p_times = 5 ### RL states

        self.GAMMA = 0.99
        self.BATCH_SIZE = 16
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 150000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
		nn.Linear(obs_len * previous_state_used, 512),
		nn.ReLU(),
		nn.Linear(512, 256),
		nn.ReLU(),
		nn.Linear(256, 128),
		nn.ReLU(),
		nn.Linear(128, actions_n)
		
		Trigger: max_episode_steps
		{'Engine': 131, 'Engine_max_cycle': 263, 'Action': <Actions.Nothing: 0>, 'offset': 147, 'state_range': '[142, 147]'}
		Step count: 1000, do_nothing count: 952, lubrication count: 45, replacment count: 3
		858101: done 988 games, reward -346.43, eps 0.01, speed 126.97614247615986 f/s
		Trigger: max_episode_steps
		{'Engine': 216, 'Engine_max_cycle': 190, 'Action': <Actions.Nothing: 0>, 'offset': 77, 'state_range': '[72, 77]'}
		Step count: 1000, do_nothing count: 938, lubrication count: 60, replacment count: 2
		859101: done 989 games, reward -325.67, eps 0.01, speed 131.34109327366275 f/s
	
0903_exp_2
		減小網路結構
		
		### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -200
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -2000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 200
        self.previous_p_times = 5 ### RL states

        self.GAMMA = 0.99
        self.BATCH_SIZE = 16
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 150000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01
		
		nn.Linear(obs_len * previous_state_used, 256),
		nn.ReLU(),
		nn.Linear(256, 256),
		nn.ReLU(),
		nn.Linear(256, actions_n),
		
		Trigger: max_episode_steps
		{'Engine': 94, 'Engine_max_cycle': 209, 'Action': <Actions.Nothing: 0>, 'offset': 145, 'state_range': '[140, 145]'}
		Step count: 1000, do_nothing count: 950, lubrication count: 46, replacment count: 4
		824462: done 931 games, reward -470.63, eps 0.01, speed 162.69259143337806 f/s
		Trigger: Engine Failure!!!
		Step count: 437, do_nothing count: 422, lubrication count: 14, replacment count: 1
		824899: done 932 games, reward -490.57, eps 0.01, speed 144.95281829110425 f/s
		Trigger: max_episode_steps
		{'Engine': 203, 'Engine_max_cycle': 225, 'Action': <Actions.Nothing: 0>, 'offset': 29, 'state_range': '[24, 29]'}
		Step count: 1000, do_nothing count: 945, lubrication count: 50, replacment count: 5
		825899: done 933 games, reward -496.0, eps 0.01, speed 165.10595636991778 f/s

0903_exp_3
		提高batch_size -> 64

        ### RL environment settings
        self.LUBRICATION_LOOKBACK = 10
        self.LUBRICATION_REWARD = -5
        self.REPLACEMENT_REWARD = -200
        self.DO_NOTHING_REWARD = 1
        self.FAILURE_REWARD = -2000

        ### RL strategy settings
        self.DEVICE = 'cpu'
        self.MEAN_REWARD_BOUND = 200
        self.previous_p_times = 5 ### RL states

        self.GAMMA = 0.99
        self.BATCH_SIZE = 64
        self.REPLAY_SIZE = 10000
        self.LEARNING_RATE = 1e-4
        self.SYNC_TARGET_FRAMES = 10000
        self.REPLAY_START_SIZE = 10000

        self.EPSILON_DECAY_LAST_FRAME = 150000
        self.EPSILON_START = 1.0
        self.EPSILON_FINAL = 0.01